{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8163d05f-501c-42d7-b0a5-5bfa8b1d56cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from pytorch_metric_learning.distances import CosineSimilarity\n",
    "from pytorch_metric_learning.utils import common_functions as c_f\n",
    "from pytorch_metric_learning.utils.inference import InferenceModel, MatchFinder\n",
    "import features\n",
    "import models\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7ccd9f9-e0ef-48b2-b07c-abfdc538d843",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa6b7489-0e3d-4561-825d-7d2ef1b8197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_decision(is_match):\n",
    "    if is_match:\n",
    "        print(\"Same class\")\n",
    "    else:\n",
    "        print(\"Different class\")\n",
    "\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-m / s for m, s in zip(mean, std)], std=[1 / s for s in std]\n",
    ")\n",
    "\n",
    "\n",
    "def imshow(img, figsize=(8, 4)):\n",
    "    img = inv_normalize(img)\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3439c243-622f-4638-a1b0-31d4febe3942",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2a8c711-9679-45af-aae6-a8e35e8a0b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = features.PypyrusDataset(data='/Users/beantown/PycharmProjects/master-thesis/data/processed/06_dataset_3/val',\n",
    "                               csv='/Users/beantown/PycharmProjects/master-thesis/data/processed/06_dataset_3/info.csv',\n",
    "                               mode='val',\n",
    "                               transform=transform,\n",
    "                               debug=False,\n",
    "                               batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2c76de6-a879-4073-bc19-902e8153b8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_indices = c_f.get_labels_to_indices(dataset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e24b0650-138e-49cd-b78a-6803f3b2ea4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunk = torchvision.models.densenet121(pretrained=False)\n",
    "trunk_output_size = trunk.classifier.in_features\n",
    "trunk.classifier = c_f.Identity()\n",
    "trunk.load_state_dict(torch.load(\"/Users/beantown/PycharmProjects/master-thesis/results/ms1/11_Dataset_3_Scheduler_Densenet121_MultiSimilarityLoss_MultiSimilarityMiner/saved_models/trunk_best13.pth\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acd329c0-87cb-46d0-9c3d-df3ea17ae6bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder = models.MLP([trunk_output_size, 64]).to(torch.device('cpu'))\n",
    "embedder.load_state_dict(torch.load(\"/Users/beantown/PycharmProjects/master-thesis/results/ms1/11_Dataset_3_Scheduler_Densenet121_MultiSimilarityLoss_MultiSimilarityMiner/saved_models/embedder_best13.pth\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c69f3c4-9c8b-4f5e-b016-0b397ca4c5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = models.MLP([64, 50]).to(torch.device('cpu'))\n",
    "classifier.load_state_dict(torch.load(\"/Users/beantown/PycharmProjects/master-thesis/results/ms1/11_Dataset_3_Scheduler_Densenet121_MultiSimilarityLoss_MultiSimilarityMiner/saved_models/classifier_best13.pth\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab975884-250c-4ff1-911c-e8131cc3b1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cars and frogs\n",
    "classA, classB = labels_to_indices[13796], labels_to_indices[1402]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29c551f5-f4a0-47aa-8589-17e1069cc78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_to_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f79cd665-bc5a-4feb-9d3b-c86ec3353a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2de7e863-791d-44d2-8599-9a3e48d33441",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "\n",
    "\n",
    "#image = io.imread('/Users/beantown/PycharmProjects/master-thesis/data/processed/03_post_cleansing/0_13927_4814C4V.png')\n",
    "image = PIL.Image.open('/Users/beantown/PycharmProjects/master-thesis/data/processed/03_post_cleansing/0_13927_4814C4V.png')\n",
    "#io.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce750f82-ca80-4622-8eef-35f902d36238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6647c0c1-2546-441c-b0b3-0e0088e35d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "396bd4eb-5b72-4193-8ad5-9ed7549ab5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2557, 3544])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fbc19858-4884-4ce7-a8ea-fd00526bc898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(images):\n",
    "    \n",
    "    transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
    "    x = transform(image)\n",
    "    x = x[None, :]\n",
    "    \n",
    "    trunk.eval()\n",
    "    embedder.eval()\n",
    "    classifier.eval()\n",
    "\n",
    "    \n",
    "    x = trunk(x)\n",
    "    x = embedder(x)\n",
    "    logits = classifier(x)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    return probs.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5a8fa228-6419-4b3c-afc0-ff1331270797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0b25cb7c-3003-4634-9561-ab433d5eba13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = batch_predict([pill_transf(image)])\n",
    "test_pred.squeeze().argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "790cc660-3f54-4fa1-9737-2b5e9db54d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "85b2de19-286f-439e-8e1e-c843d477b19c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Buffer dtype mismatch, expected 'double' but got 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wj/vxbvyct136sdcvzl02f2nkph0000gn/T/ipykernel_16524/2210836641.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlime_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLimeImageExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m explanation = explainer.explain_instance(x.T, \n\u001b[0m\u001b[1;32m      3\u001b[0m                                          \u001b[0mbatch_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# classification function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                          \u001b[0mtop_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                          \u001b[0mhide_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/model/lib/python3.8/site-packages/lime/lime_image.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[0;34m(self, image, classifier_fn, labels, hide_color, top_labels, num_features, num_samples, batch_size, segmentation_fn, distance_metric, model_regressor, random_seed)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0msegments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegmentation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mfudged_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/model/lib/python3.8/site-packages/lime/lime_image.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[0;34m(self, image, classifier_fn, labels, hide_color, top_labels, num_features, num_samples, batch_size, segmentation_fn, distance_metric, model_regressor, random_seed)\u001b[0m\n\u001b[1;32m    180\u001b[0m                                                     random_seed=random_seed)\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0msegments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegmentation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/model/lib/python3.8/site-packages/lime/wrappers/scikit_image.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/miniconda3/envs/model/lib/python3.8/site-packages/skimage/segmentation/_quickshift.py\u001b[0m in \u001b[0;36mquickshift\u001b[0;34m(image, ratio, kernel_size, max_dist, return_tree, sigma, convert2lab, random_seed)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     segment_mask = _quickshift_cython(\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_dist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_dist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         return_tree=return_tree, random_seed=random_seed)\n",
      "\u001b[0;32mskimage/segmentation/_quickshift_cy.pyx\u001b[0m in \u001b[0;36mskimage.segmentation._quickshift_cy._quickshift_cython\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Buffer dtype mismatch, expected 'double' but got 'float'"
     ]
    }
   ],
   "source": [
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanation = explainer.explain_instance(x.T, \n",
    "                                         batch_predict, # classification function\n",
    "                                         top_labels=5, \n",
    "                                         hide_color=0, \n",
    "                                         num_samples=1000) # number of images that will be sent to classification function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be32a43-fb25-42f8-95e7-a6c150e79294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model",
   "language": "python",
   "name": "model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
